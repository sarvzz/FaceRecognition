{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37f95bc-e948-4588-8ae8-600948ff0981",
   "metadata": {},
   "source": [
    "# 1. Setup and Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bdf09f-ab50-42ef-858a-d19bcf9dc7b6",
   "metadata": {},
   "source": [
    "### 1.1 Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f8f499-4124-44b4-a974-2d6a356cb4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install labelme tensorflow opencv-python matplotlib albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c4670-f38c-4b01-8efe-58bb7188137a",
   "metadata": {},
   "source": [
    "### 1.2 Collect Images Using OpenCV (**Optional**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73c3039-2c40-430a-8a7d-10c46be6b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8959ddbb-9e26-4c68-af43-b03e9e24dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = os.path.join('data','images')\n",
    "number_images = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2535889-ef25-4782-b5dc-2ff3a84866e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "for imgnum in range(number_images):\n",
    "    print('collecting image {}'.format(imgnum))\n",
    "    ret, frame = cap.read()\n",
    "    imgname = os.path.join(IMAGES_PATH, f'{str(uuid.uuid1())}.jpg')\n",
    "    cv2.imwrite(imgname, frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b520281-4d31-4e2b-bb0d-0cbc7efe604d",
   "metadata": {},
   "source": [
    "### 1.3 Annotate Images with LabelMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e957fb-228b-4115-a785-14cac76bd0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32f060-cdb8-48cc-8a0e-ce02972f90a0",
   "metadata": {},
   "source": [
    "# 2. Review Dataset and Build Image Loading Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac677382-a0bd-4005-9a13-fde86a831fb2",
   "metadata": {},
   "source": [
    "### 2.1 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444eaf2c-b877-4e7b-b897-4a584c9d33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bf3e1-741b-47ca-81e9-6d1cf79a93f9",
   "metadata": {},
   "source": [
    "### 2.2 Load Image into TF Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e8ff9-10b4-4c8b-b85e-3deeeb61daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa693dc4-364d-4393-b888-d10de0422bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(x): \n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7a473-fb93-44e0-b10e-ecb8efe81401",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e5e8e-d8a6-4045-b777-1bc8d7fb4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c08a5-e937-4690-8b24-da2fc27b4366",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c01f4-e1e8-4364-ae95-60b3b6bc84ce",
   "metadata": {},
   "source": [
    "### 2.3 View Raw Images with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc4f57-da3e-4711-be5b-b6c04a0892df",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = images.batch(2).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb872cce-81e3-4e47-a217-adbe0a8455a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images = image_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d4327-416f-467f-a9a8-32edbb320aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(20,20))\n",
    "for idx, image in enumerate(plot_images):\n",
    "    ax[idx].imshow(image) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34aa05d-5dfd-4319-a5e5-3d97632e585d",
   "metadata": {},
   "source": [
    "# 3. Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e10a93-e59e-411c-8cfd-fa9a1330594b",
   "metadata": {},
   "source": [
    "### 3.1 Move the Matching Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4afc2c-537c-4380-9c30-43a6681713e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['train', 'test', 'val']:\n",
    "    for file in os.listdir(os.path.join(data, folder, 'face_image')):\n",
    "\n",
    "        filename = file.split('.')[0]+'.json'\n",
    "        existing_filepath = os.path.join('data','face_labels',filename)\n",
    "        if os.path.exists(existing_filepath):\n",
    "            new_filepath = os.path.join('data', folder,'face_labels', filename)\n",
    "            os.replace(existing_filepath, new_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb47d0-e2f3-4d9a-a64b-95005f1508b1",
   "metadata": {},
   "source": [
    "# 4. Augment Images & Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473764cc-1b2b-406a-bd2d-722b86dda282",
   "metadata": {},
   "source": [
    "### 4.1 Setup Albumentations Transform Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b275c99c-6651-48f5-8553-bc84e1f9ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725f05d-4bc0-4f03-97b8-38a9bf2fe81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.CenterCrop(width=450, height=450),\n",
    "                         alb.HorizontalFlip(p=0.5),\n",
    "                         alb.RandomBrightnessContrast(p=0.25),\n",
    "                         alb.RandomGamma(p=0.25),\n",
    "                         alb.RGBShift(p=0.25),\n",
    "                         alb.VerticalFlip(p=0.5)],\n",
    "                         bbox_params=alb.BboxParams(format='albumentations', \n",
    "                                                  label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3a075-5445-409e-adbb-85bb7916da54",
   "metadata": {},
   "source": [
    "### 4.2 Load a Test Image and Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f74ce-a6c2-4df5-8dc4-32fa256dc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join('data', 'face_image', 'train', 'img_1.jpg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f4ac4-aebd-41e5-8c35-97750b88252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'face_labels', 'train', 'img_1.json'), 'r') as f:\n",
    "    label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50ae91-ff8e-408e-8608-9ff4d952a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label['shapes'][0]['points']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d4439-bf53-4226-a636-fb1d08d0684f",
   "metadata": {},
   "source": [
    "### 4.3 Extract Coordinates and Rescale to Match Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643926ed-d82c-436f-936c-686f905b9a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [0,0,0,0]\n",
    "coords[0] = label['shapes'][0]['points'][0][0]\n",
    "coords[1] = label['shapes'][0]['points'][0][1]\n",
    "coords[2] = label['shapes'][0]['points'][1][0]\n",
    "coords[3] = label['shapes'][0]['points'][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a15fc1-7be1-40bf-bc84-1ed81c054e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf6f10-02aa-4aa6-8537-24b9217c3508",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = list(np.divide(coords, [900,900,900,900])) #[1280,720,1280,720]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38784109-184a-4841-ad82-d095cd3393e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636dbfbb-46cf-4256-bba0-a2b7c34d22c5",
   "metadata": {},
   "source": [
    "### 4.4 Apply Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc974d8-4ca3-495a-9fde-06f702bc8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d82851-d3e9-4957-88fb-b5dd0b19d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes'][0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e58d5-69bf-4722-8cf5-e81bfded2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036fc1d0-4523-40c0-ba0b-31e2f1c9f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(augmented['image'],\n",
    "              tuple(np.multiply(augmented['bboxes'][0][:2], [450,450]).astype(int)),\n",
    "                tuple(np.multiply(augmented['bboxes'][0][2:], [450,450]).astype(int)),\n",
    "                    (255,0,0), 2)\n",
    "\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f83af4-3110-4290-95eb-94afc4162103",
   "metadata": {},
   "source": [
    "# 5. Build & Run Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647a251-6f5b-4653-8155-6d090c3573f3",
   "metadata": {},
   "source": [
    "### 5.1 Run Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba9da5-c170-482a-8206-4208a0cd04f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in ['train','test','val']: \n",
    "    for image in os.listdir(os.path.join('data', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join('data', partition, 'images', image))\n",
    "\n",
    "        coords = [0,0,0.00001,0.00001]\n",
    "        label_path = os.path.join('data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "\n",
    "            coords[0] = label['shapes'][0]['points'][0][0]\n",
    "            coords[1] = label['shapes'][0]['points'][0][1]\n",
    "            coords[2] = label['shapes'][0]['points'][1][0]\n",
    "            coords[3] = label['shapes'][0]['points'][1][1]\n",
    "            coords = list(np.divide(coords, [1280,720,1280,720]))\n",
    "\n",
    "        try: \n",
    "            for x in range(60):\n",
    "                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
    "                cv2.imwrite(os.path.join('aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0: \n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0 \n",
    "                    else: \n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else: \n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0 \n",
    "\n",
    "\n",
    "                with open(os.path.join('aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a26e33-aac6-4cbb-98b0-13fa3400b826",
   "metadata": {},
   "source": [
    "### 5.2 Load Augmented Images to Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811379e-8006-42fb-9e41-0ba8236da553",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('aug_data/train/images/*.jpg', shuffle=False)\n",
    "train_images = train_images.map(load_image)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a807fe2-5a40-4dc1-ab59-6590c4cc44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_data/test/images/*.jpg', shuffle=False)\n",
    "test_images = test_images.map(load_image)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab1fe0-2791-49f0-aec4-8310fc80bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_data/val/images/*.jpg', shuffle=False)\n",
    "val_images = val_images.map(load_image)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868777a3-2db1-407d-bf40-2c1eb75377c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea955aa-dca3-4666-bc62-820fd7a8b92a",
   "metadata": {},
   "source": [
    "# 6. Prepare Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b639ce6b-5694-464a-b0d2-a02ea90d6e0f",
   "metadata": {},
   "source": [
    "### 6.1 Build Label Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0490c17-3c77-44dc-a5bc-6668cd956e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding='utf-8') as f:\n",
    "        label = json.load(f)\n",
    "\n",
    "    return [label['class'], label['bbox']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6057590-fc0c-4969-8e15-e0da92ff1975",
   "metadata": {},
   "source": [
    "### 6.2 Load Labels to Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61adda-8024-4b91-80fe-68b85d13838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_data/train/labels/*.json', shuffle=False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e196b63-19ce-4c90-89f7-da8ce780bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files('aug_data/test/labels/*.json', shuffle=False)\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c902d4e-11ac-4423-b4fe-2f5605ef6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files('aug_data/val/labels/*.json', shuffle=False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ae998-78ee-4f70-a82f-f4050a231ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eda743-192a-4fc8-8ae1-1dc66a902b60",
   "metadata": {},
   "source": [
    "# 7. Combine Label and Image Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df465362-22d2-4617-8b7d-d7750fcc83ee",
   "metadata": {},
   "source": [
    "### 7.1 Check Partition Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f88a4-cec9-4101-abc2-1c9f45a41a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc640a-6765-45db-a4c3-a96fd0856a55",
   "metadata": {},
   "source": [
    "### 7.2 Create Final Datasets (Images/Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de037c8-197e-431b-9cc2-5d1828797c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(300)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7a9b7-6c44-417b-a180-60b774a3cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(60)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4fa35-2ce9-462b-8fb0-6df05f31dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(100)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda32dd-62ab-42d6-845d-cc55081a2e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.as_numpy_iterator().next()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e396b13-5a9f-48a9-9d44-2de632c94a57",
   "metadata": {},
   "source": [
    "### 7.3 View Images and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a77947-4701-47d5-b37c-e02a18a5d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cabe0c-feb9-4d40-b053-5bcb668217f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = data_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d13151-f212-4fe6-93ae-77aa1fa95f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = res[0][idx]\n",
    "    sample_coords = res[1][1][idx]\n",
    "    \n",
    "    cv2.rectangle(sample_image, \n",
    "                  tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                        (255,0,0), 2)\n",
    "\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02ccb4-71da-43da-bde4-e344f7f58f9e",
   "metadata": {},
   "source": [
    "# 8. Build Deep Learning using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a6821-2f25-40a1-a2d8-962edfad955e",
   "metadata": {},
   "source": [
    "### 8.1 Import Layers and Base Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713db71-6f4a-4272-92db-48617c16d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.,applications import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ce676-c813-40b9-9a1b-db1eb9e3067c",
   "metadata": {},
   "source": [
    "### 8.2 Download VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c8a75-d5e9-4fe1-a7b0-d0f5e47b6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b9f77-3b0a-45ba-8f6c-4e73a4c54e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad646739-0628-401c-bc04-aa6063b2c138",
   "metadata": {},
   "source": [
    "### 8.3 Build instance of Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f3f5b-6b0d-4d4e-8e59-6a1fedf51919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_layer = Input(shape=(120,120,3))\n",
    "\n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "\n",
    "    # Classification Model  \n",
    "    f1 = GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(2048, activation='relu')(f1)\n",
    "    class2 = Dense(1, activation='sigmoid')(class1)\n",
    "\n",
    "    # Bounding box model\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4, activation='sigmoid')(regress1)\n",
    "\n",
    "    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d2b015-0524-4a94-83d4-6bfd6ec97f92",
   "metadata": {},
   "source": [
    "### 8.4 Test out Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8773faf-72e9-4967-9829-c0c844c19594",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83dc3ca-b16f-45ae-8c2d-7a9e1c63956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607776ac-c64c-4ca8-9272-6d15cc0c3ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fadc151-4ce9-4649-ba52-43f45a09228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a88930e-574f-4d8d-bafe-46635a606141",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords = facetracker.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f2fd33-b618-424d-ad22-6d5574947747",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29872181-334e-413f-8e65-b76d057de72e",
   "metadata": {},
   "source": [
    "# 9. Define Losses and Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328c279c-c43d-4bed-893f-9bc38022bfed",
   "metadata": {},
   "source": [
    "### 9.1 Define Optimizer and LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d2162-353f-4052-8f61-9fd047ce916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train)\n",
    "lr_decay = (1/0.75-1)/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7166cc-b23c-40d4-b663-36da96663d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98c950-d4b3-4275-a4b2-e142f9c2a95c",
   "metadata": {},
   "source": [
    "### 9.2 Create Localization Loss and Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1f064-1afd-44e1-b6be-341a56eafef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):\n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2]-yhat[:,:2]))\n",
    "\n",
    "    h_true = y_true[:,3] - y_true[:,1]\n",
    "    w_true = y_true[:,2] - y_true[:,0]\n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:,1]\n",
    "    w_pred = yhat[:,2] yhat[:,0]\n",
    "\n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true - h_pred))\n",
    "    \n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7424a4-e1a1-4d08-84cf-b70c26c67172",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss = tf.keras.losses.BinaryCrossentropy()\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb9e36-a4bb-47fb-9549-12fbb61e0e35",
   "metadata": {},
   "source": [
    "### 9.3 Test out Loss Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495fd41-8400-4593-9a5d-63018614bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_loss(y[1], coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03bde7-fe48-485e-9dca-49ffea761819",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss(y[0], classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c59fd0d-f251-4e55-8ea7-ed4f76676159",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressloss(y[1], coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075b87f-9513-4032-9abc-cfd0928be8f4",
   "metadata": {},
   "source": [
    "# 10. Train Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b46afe-e72e-403d-87e1-895a0fe266c5",
   "metadata": {},
   "source": [
    "### 10.1 Create Custom Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61822b2b-14ca-444c-87f0-049f0e04b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model):\n",
    "    def __init__(self, eyetracker, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = eyetracker\n",
    "\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "        self.opt = opt\n",
    "\n",
    "    def train_step(self, batch, **kwargs):\n",
    "        X, y = batch\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            classes, coords = self.model(X, training=True)\n",
    "\n",
    "            batch_classloss = self.closs(y[0], classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "            \n",
    "            total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "            \n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        \n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "    \n",
    "    def test_step(self, batch, **kwargs): \n",
    "        X, y = batch\n",
    "        \n",
    "        classes, coords = self.model(X, training=False)\n",
    "        \n",
    "        batch_classloss = self.closs(y[0], classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "        \n",
    "    def call(self, X, **kwargs): \n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb5b93-bcf2-4de3-9e43-41459469d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceTracker(facetracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbbfe7d-f3d5-4782-bd30-bc6c9f9bc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt, classloss, regressloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21390031-f963-44da-bdfe-35a9803da524",
   "metadata": {},
   "source": [
    "### 10.2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1581f-9e93-4a67-9983-8c895675d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'logs' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbacf5f7-49eb-4282-adf4-78f998777a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59bf17-5749-489c-837c-6fd704838141",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = model.fit(train, epoch=10, validation_data= val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2b405-4b24-44b5-8b1f-e243a19597c1",
   "metadata": {},
   "source": [
    "### 10.3 Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39d8c5-99d1-4044-9f7b-7dff11798949",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e259af6-19db-47d7-b1d2-756e240a4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33949103-5415-41c5-a3bb-7f3494b80d31",
   "metadata": {},
   "source": [
    "# 11. Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e89c50-bbe9-4451-9000-12fe2f8f5958",
   "metadata": {},
   "source": [
    "### 11.1 Make Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf284d-589b-40d0-92f4-9d182e96e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68feb2ac-21b0-4d92-9605-d86a800c84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a267eeb-a830-417d-a6bd-2d9be30a8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = facetracker.predict(test_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63c12a-9adb-46cf-8a63-ffe9a87ce5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize(20,20))\n",
    "\n",
    "for idx in range(4):\n",
    "    sample_image = test_sample[0][idx]\n",
    "    sample_coords = yhat[1][idx]\n",
    "\n",
    "    if yhat[0][idx] > 0.9:\n",
    "        cv2.rectangle(sample_image,\n",
    "                     tuple(np.multiply(sample[:2], [120,120].astype(int))),\n",
    "                     tuple(np.multiply(sample_coords[2:], [120,120].astype(int))),\n",
    "                     (255,0,0), 2)\n",
    "\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05273397-9d52-416a-805c-3496957d68c6",
   "metadata": {},
   "source": [
    "### 11.2 Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b903a240-90ca-4ffd-843a-5613583e9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ec193-e163-4bbd-9872-408cf05294b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.save('facetracker.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf49887-25a5-4820-947a-0623984dcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = load_model('facetracker.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233ace1-ddc4-4db0-ac8f-59683f3fe574",
   "metadata": {},
   "source": [
    "### 11.3 Real Time Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b5da7-eaa9-49e7-a6fc-a5d6134da0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCaprture(1)\n",
    "\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500, :]\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "\n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "\n",
    "    if yhat[0] > 0.5:\n",
    "        # Controls the main rectangle:\n",
    "        cv2.rectangle(frame, \n",
    "                     tuple(np.multiply(sample[:2], [450,450]).astype(int)),\n",
    "                     tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)),\n",
    "                     (2255,0,0), 2)\n",
    "        \n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame,\n",
    "                     tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int)\n",
    "                                 [0,-30])),\n",
    "                     tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                 [80,0])),\n",
    "                     (255,0,0), -1)\n",
    "\n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(multiply(sample_coords[:2], [450,450]).astype()int),\n",
    "                                        [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('facetracker', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
